{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hello world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from transition_amr_parser.parse import AMRParser\n",
    "from transition_amr_parser.amr import AMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../dataset/dataset_multilingual.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EN'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "languages = set(df['language'])\n",
    "languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df['en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Madam President, on a point of order. You will be aware from the press and television that there have been a number of bomb explosions and killings in Sri Lanka. One of the people assassinated very recently in Sri Lanka was Mr Kumar Ponnambalam, who had visited the European Parliament just a few months ago. Would it be appropriate for you, Madam President, to write a letter to the Sri Lankan President expressing Parliament's regret at his and the other violent deaths in Sri Lanka and urging her to do everything she possibly can to seek a peaceful reconciliation to a very difficult situation?\n",
      "\n",
      "\n",
      "\n",
      "Madam President, on a point of order. I would like your advice about Rule 143 concerning inadmissibility. My question relates to something that will come up on Thursday and which I will then raise again.\n",
      "\n",
      "The Cunha report on multiannual guidance programmes comes before Parliament on Thursday and contains a proposal in paragraph 6 that a form of quota penalties should be introduced for countries which fail to meet their fleet reduction targets annually. It says that this should be done despite the principle of relative stability. I believe that the principle of relative stability is a fundamental legal principle of the common fisheries policy and a proposal to subvert it would be legally inadmissible. I want to know whether one can raise an objection of that kind to what is merely a report, not a legislative proposal, and whether that is something I can competently do on Thursday.\n",
      "\n",
      "\n",
      "\n",
      "Madam President, can you tell me why this Parliament does not adhere to the health and safety legislation that it actually passes? Why has no air quality test been done on this particular building since we were elected? Why has there been no Health and Safety Committee meeting since 1998? Why has there been no fire drill, either in the Brussels Parliament buildings or the Strasbourg Parliament buildings? Why are there no fire instructions? Why have the staircases not been improved since my accident? Why are no-smoking areas not enforced? It seems absolutely disgraceful that we pass legislation and do not adhere to it ourselves.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Madam President, I should like to know if there will be a clear message going out from Parliament this week about our discontent over today's decision refusing to renew the arms embargo on Indonesia, considering that the vast majority in this Parliament have endorsed the arms embargo in Indonesia in the past? Today's decision not to renew the embargo is extremely dangerous considering the situation there. So Parliament should send a message, since that is the wish of the vast majority. It is irresponsible of EU Member States to refuse to renew the embargo. As people have said, the situation there is extremely volatile. There is, in fact, a risk of a military coup in the future. We do not know what is happening. So why should EU arms producers profit at the expense of innocent people?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Madam President, in the earlier vote - and I will abide by your ruling on this matter - on the question of the strategic plan of the Commission I indicated that I would like to speak in advance of the vote on behalf of my Group. That did not happen. I would appreciate it if, on the close of this item of business, I might be allowed to give an explanation of vote on behalf of my Group. This is an important matter. It would be useful for the record of the House to state how people perceive what we have just done in the light of their own political analysis.\n",
      "\n",
      "\n",
      "\n",
      "Madam President, if the vote records correctly how my Group voted I shall not, and cannot, object to that. If your ruling is that I cannot give an explanation of vote, I accept that but with reservations.\n",
      "\n",
      "\n",
      "\n",
      "Madam President, first of all I should like to thank Mr Koch for his report which has, at its heart, the issue of transport safety. The report looks at the issue of harmonising the examination requirements for safety advisors working in the areas of transportation of dangerous goods by road, rail and inland waterway. I congratulate him on his excellent report.\n",
      "\n",
      "Transport safety has sadly been in the news recently: the Paddington rail crash in London, the terrible rail crash in Norway, the two aviation crashes involving EU citizens and the natural disaster involving the Erika off Brittany - all within the last four months - remind us that transport safety can never be taken for granted and that those charged with protecting the public must be highly motivated and highly qualified.\n",
      "\n",
      "The rapporteur has pointed out to the House that in its common position the Council has accepted six of Parliament's ten amendments put forward at first reading and that the substance of Parliament's other amendments has been retained. My Group will therefore support the common position and looks forward to the enactment of the legislation which will provide us with yet another tool in our fight to make transport in the European Union as safe as possible. When it comes to safety my Group will always support any initiatives to improve transport safety. We still have a lot of work to do in this area as recent events have proved.\n",
      "\n",
      "\n",
      "\n",
      "Mr President, it is particularly pleasing for me to make my first speech in the European Parliament on what is regarded as the most important issue within that part of the United Kingdom that I represent in this Parliament, namely Wales.\n",
      "\n",
      "A major part of Wales, as you know, has been granted Objective 1 status under the Structural Funds programme. It is quite clear that many people within Wales are looking to the European Structural Funds programme to alleviate some of the great difficulties that we undoubtedly face. We have seen poverty growing in Wales; and growing still further since 1997. We have seen the gap between rich and poor widen. We are looking, therefore, within the Structural Funds programme not just to see industrial restructuring but also to see a wider improvement in the whole of the economic base within the Principality.\n",
      "\n",
      "What is, however, deeply damaging for us is the belief that in some way the granting of Structural Funds assistance is something that has been, in a sense, a success of the government. It is sadly only a recognition of the very great difficulties that Wales faces. That is why I want to highlight some of the issues that I believe the Commission must have at the forefront.\n",
      "\n",
      "We look to the Commission to deal with points in relation to additionality. We are dissatisfied with the fact that those figures seem to have been in some way hidden within UK figures. We look to the Commission also to ensure that there is matched funding for projects. We look to it to challenge the UK Government, to ensure that the private sector, which surely must be providing the major impetus for Structural Funds expenditure, is involved in the planning stage. Finally, we ask that the Commission ensures that Structural Fund monies are spent in a way which is transparent. Too much of what takes place within this Parliament is not transparent. This is one area in which I believe the Commission can be a very great friend to Wales.\n",
      "\n",
      "\n",
      "\n",
      "Mr President, I would very much like to thank Mrs Schroedter for the work she has done on this and to explain to colleagues that I am speaking for my colleague, Mrs Flautre, who followed this for the Committee on Employment and Social Affairs but who is unfortunately ill.\n",
      "\n",
      "I would like to draw people's attention to Amendments Nos 1 and 2 which were agreed by the Committee on Employment and Social Affairs but not accepted by the Committee on Regional Policy, Transport and Tourism. These amendments deal with the social economy and the need to provide social risk capital and support financially local schemes to develop employment opportunities and strengthen social cohesion.\n",
      "\n",
      "In the past, this Parliament has viewed the social economy as an important potential provider of employment. These amendments also fit in with this Parliament's view that social exclusion is a serious issue needing constructive action. We hope that those considering rejection of these amendments have very powerful reasons to offer to both Parliament and their citizens who are seeking employment.\n",
      "\n",
      "In her report, Mrs Flautre also drew attention to an area where coordination is sorely lacking, yet desperately needed. The Commission proposals refer to the four pillars of employment strategy and the five fields of action of the European Social Fund. But the lack of specific guidelines here is particularly to be regretted, as the idea of linking Social Fund assistance to the employment strategy will be put into effect for the first time during the 2000-2006 programme. It could be said that the omission gives the impression that the Commission too has no idea how to provide maximum coordination between European Social Fund assistance, which is subject to review after three and a half years, and the Member States' annual national plans for employment. We hope that the Commission can reassure us that this was an oversight which is now being dealt with constructively.\n",
      "\n",
      "\n",
      "\n",
      "Mr President, it is incumbent upon me to remind my colleague, Mr Evans, of why Wales actually achieved Objective 1 status. It was because of the discredited policies of his own Conservative Party. Let me also remind him that when his party leader, Mr Hague, was Secretary of State for Wales, he broke every rule in the book on additionality which led to a stern letter from Commissioner Wulf-Mathies regarding regulatory requirements. I can tell you that the British Government is aware of its regulatory requirements on Objective 1 additionality. I suggest Mr Evans goes back and reads the regulation.\n",
      "\n",
      "My Group has made extensive amendments to both reports up for debate today. I want to focus our minds on the essential role of the guidelines. The objective is to provide a framework and tool to support and enhance economic regeneration, to get the most effective use of resources in the widest partnership and to put these regions back on the road to recovery and sustainable development so that eventually they come off the regional life-support machine.\n",
      "\n",
      "It is important to identify the skills and potential of our regions in the hi-tech sector. It is particularly important in the light of reports in the media that Europe is rapidly losing ground to the US in the hi-tech growth industries of the future.\n",
      "\n",
      "The operation of the previous round of programmes is also very instructive in telling us what guidelines should not be about. They should not be about creating additional layers of bureaucracy and red tape. They should not be about shifting priorities and policies halfway through project development, resulting in inevitable delays and underspends, particularly in the light of the new budgetary requirement.\n",
      "\n",
      "The implementation and operation of the guidelines cannot be left to the personal interpretation of one or other desk officer, either in the Commission or in the civil service. There must be an internal coherence in the Commission directorate, while respecting the specific local and regional aspects of Commission programmes.\n",
      "\n",
      "The conclusion is that we must make the case for guidelines to be broad, indicative and flexible to assist our programme managers and fund-users and to get the maximum potential out of our new fields of regeneration. If we can inject a spirit of entrepreneurial activity into our poor and structurally weak regions we will eventually get them back onto the road of attracting substantial investor confidence, which will be the key to future success. This is how we are going to judge the success of these guidelines: whether EU regional policy with a good, solid, enabling guideline, can open up new opportunities and allow our poor and structurally weak regions to play their full part in contributing to the growth and prosperity of the EU.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splits(text):\n",
    "    sentences = re.split(r'([.!?])', text)\n",
    "    sentences = [sentences[i] + sentences[i+1] for i in range(0, len(sentences)-1, 2)]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(map(splits, sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\nMadam President, on a point of order.', ' You will be aware from the press and television that there have been a number of bomb explosions and killings in Sri Lanka.', ' One of the people assassinated very recently in Sri Lanka was Mr Kumar Ponnambalam, who had visited the European Parliament just a few months ago.', \" Would it be appropriate for you, Madam President, to write a letter to the Sri Lankan President expressing Parliament's regret at his and the other violent deaths in Sri Lanka and urging her to do everything she possibly can to seek a peaceful reconciliation to a very difficult situation?\"]\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load from cache /Users/cricoche/.cache/torch/DATA/AMR3.0/models/amr3.0-structured-bart-large-neur-al/seed42/checkpoint_wiki.smatch_top5-avg.pt\n",
      "| [en] dictionary: 46088 types\n",
      "| [actions_nopos] dictionary: 16544 types\n",
      "----------loading pretrained bart.large model ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/cricoche/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- task bart rewind: loading pretrained bart.large model ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/cricoche/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using CPU for models\n",
      "pretrained_embed:  bart.large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/cricoche/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using bart.large extraction in cpu (slow, wont OOM)\n",
      "Finished loading models\n",
      "self.machine_config:  /Users/cricoche/.cache/torch/DATA/AMR3.0/models/amr3.0-structured-bart-large-neur-al/seed42/machine_config.json\n"
     ]
    }
   ],
   "source": [
    "parser = AMRParser.from_pretrained('AMR3-structbart-L')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on batch size: 1\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decoding:   0%|                                                                         | 0/1 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[164], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m tokens, positions \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mtokenize(sentence)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Parse the sentence\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m annotations, machines \u001b[38;5;241m=\u001b[39m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_sentence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Get AMR graph object\u001b[39;00m\n\u001b[1;32m     13\u001b[0m amr \u001b[38;5;241m=\u001b[39m machines\u001b[38;5;241m.\u001b[39mget_amr()\n",
      "File \u001b[0;32m~/Desktop/vscodeFiles/british/cenv_x86/lib/python3.8/site-packages/transition_amr_parser/parse.py:994\u001b[0m, in \u001b[0;36mAMRParser.parse_sentence\u001b[0;34m(self, tokens, **kwargs)\u001b[0m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_sentence\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 994\u001b[0m     annotations, decoding_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    995\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m annotations[\u001b[38;5;241m0\u001b[39m], decoding_data[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/vscodeFiles/british/cenv_x86/lib/python3.8/site-packages/transition_amr_parser/parse.py:1046\u001b[0m, in \u001b[0;36mAMRParser.parse_sentences\u001b[0;34m(self, batch, batch_size, roberta_batch_size, gold_amrs, force_actions, beam, jamr, no_isi, unicode_normalize)\u001b[0m\n\u001b[1;32m   1043\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1045\u001b[0m \u001b[38;5;66;03m# parse for this data batch\u001b[39;00m\n\u001b[0;32m-> 1046\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_amr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_amr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# collect all annotations\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_amr:\n",
      "File \u001b[0;32m~/Desktop/vscodeFiles/british/cenv_x86/lib/python3.8/site-packages/transition_amr_parser/parse.py:932\u001b[0m, in \u001b[0;36mAMRParser.parse_batch\u001b[0;34m(self, sample, to_amr)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, sample, to_amr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;66;03m# TODO: pass arbitrary machines in generator to enable inspector and\u001b[39;00m\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;66;03m# other\u001b[39;00m\n\u001b[0;32m--> 932\u001b[0m     hypos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    934\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    936\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    937\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, sample_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()):\n",
      "File \u001b[0;32m~/Desktop/vscodeFiles/british/cenv_x86/lib/python3.8/site-packages/fairseq_ext/tasks/amr_action_pointer_bart.py:548\u001b[0m, in \u001b[0;36mAMRActionPointerBARTParsingTask.inference_step\u001b[0;34m(self, generator, models, sample, args, prefix_tokens)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minference_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, generator, models, sample, args, prefix_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    547\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 548\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mrun_amr_sm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_amr_sm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mmodify_arcact_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodify_arcact_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43muse_pred_rules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_pred_rules\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/vscodeFiles/british/cenv_x86/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/vscodeFiles/british/cenv_x86/lib/python3.8/site-packages/fairseq_ext/sequence_generator.py:631\u001b[0m, in \u001b[0;36mSequenceGenerator.generate\u001b[0;34m(self, models, sample, prefix_tokens, bos_token, run_amr_sm, modify_arcact_score, use_pred_rules, **kwargs)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;66;03m# lprobs, avg_attn_scores = model.forward_decoder(\u001b[39;00m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;66;03m#     tokens[:, :step + 1], encoder_outs, temperature=self.temperature,\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m    629\u001b[0m avg_attn_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m    \u001b[38;5;66;03m# this for the cross attention on the source tokens\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m lprobs, avg_attn_tgt_scores \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_decoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokens_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_outs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mactions_states\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;66;03m# lprobs[:, self.pad] = -math.inf  # never select pad\u001b[39;00m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;66;03m# lprobs[:, self.unk] -= self.unk_penalty  # apply unk penalty\u001b[39;00m\n\u001b[1;32m    638\u001b[0m lprobs[\u001b[38;5;241m~\u001b[39mallowed_mask] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mmath\u001b[38;5;241m.\u001b[39minf\n",
      "File \u001b[0;32m~/Desktop/vscodeFiles/british/cenv_x86/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/vscodeFiles/british/cenv_x86/lib/python3.8/site-packages/fairseq_ext/sequence_generator.py:1188\u001b[0m, in \u001b[0;36mEnsembleModel.forward_decoder\u001b[0;34m(self, tokens, encoder_outs, temperature, **kwargs)\u001b[0m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_decoder\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens, encoder_outs, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 1188\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode_one\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoder_outs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhas_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincremental_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1198\u001b[0m     log_probs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1199\u001b[0m     avg_attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/vscodeFiles/british/cenv_x86/lib/python3.8/site-packages/fairseq_ext/sequence_generator.py:1226\u001b[0m, in \u001b[0;36mEnsembleModel._decode_one\u001b[0;34m(self, tokens, model, encoder_out, incremental_states, log_probs, temperature, **kwargs)\u001b[0m\n\u001b[1;32m   1221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decode_one\u001b[39m(\n\u001b[1;32m   1222\u001b[0m     \u001b[38;5;28mself\u001b[39m, tokens, model, encoder_out, incremental_states, log_probs,\n\u001b[1;32m   1223\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   1224\u001b[0m ):\n\u001b[1;32m   1225\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mincremental_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1226\u001b[0m         decoder_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mincremental_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincremental_states\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m                                         \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1228\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1229\u001b[0m         decoder_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(model\u001b[38;5;241m.\u001b[39mdecoder(tokens, encoder_out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[0;32m~/Desktop/vscodeFiles/british/cenv_x86/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/vscodeFiles/british/cenv_x86/lib/python3.8/site-packages/fairseq_ext/models/transformer_tgt_pointer_bart.py:1041\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[0;34m(self, prev_output_tokens, encoder_out, incremental_state, features_only, full_context_alignment, alignment_layer, alignment_heads, src_lengths, return_all_hiddens, tgt_vocab_masks, tgt_actnode_masks, tgt_src_cursors, **unused)\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m   1007\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1008\u001b[0m     prev_output_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused\n\u001b[1;32m   1023\u001b[0m ):\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;124;03m        prev_output_tokens (LongTensor): previous decoder outputs of shape\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;124;03m            - a dictionary with any model-specific outputs\u001b[39;00m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1041\u001b[0m     x, extra \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprev_output_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_out\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincremental_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mincremental_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfull_context_alignment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_context_alignment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m        \u001b[49m\u001b[43malignment_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malignment_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43malignment_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malignment_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# customized for APT\u001b[39;49;00m\n\u001b[1;32m   1049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtgt_src_cursors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_src_cursors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtgt_actnode_masks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_actnode_masks\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m features_only:\n\u001b[1;32m   1053\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer(\n\u001b[1;32m   1054\u001b[0m             x,\n\u001b[1;32m   1055\u001b[0m             \u001b[38;5;66;03m# customized for APT\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m             tgt_vocab_masks\u001b[38;5;241m=\u001b[39mtgt_vocab_masks\n\u001b[1;32m   1057\u001b[0m         )\n",
      "File \u001b[0;32m~/Desktop/vscodeFiles/british/cenv_x86/lib/python3.8/site-packages/fairseq_ext/models/transformer_tgt_pointer_bart.py:1072\u001b[0m, in \u001b[0;36mTransformerDecoder.extract_features\u001b[0;34m(self, prev_output_tokens, encoder_out, incremental_state, full_context_alignment, alignment_layer, alignment_heads, tgt_src_cursors, tgt_actnode_masks)\u001b[0m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_features\u001b[39m(\n\u001b[1;32m   1061\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1062\u001b[0m     prev_output_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1070\u001b[0m     tgt_actnode_masks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m ):\n\u001b[0;32m-> 1072\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_features_scriptable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprev_output_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincremental_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfull_context_alignment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m        \u001b[49m\u001b[43malignment_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m        \u001b[49m\u001b[43malignment_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# customized for APT\u001b[39;49;00m\n\u001b[1;32m   1080\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtgt_src_cursors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_src_cursors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtgt_actnode_masks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_actnode_masks\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/vscodeFiles/british/cenv_x86/lib/python3.8/site-packages/fairseq_ext/models/transformer_tgt_pointer_bart.py:1264\u001b[0m, in \u001b[0;36mTransformerDecoder.extract_features_scriptable\u001b[0;34m(self, prev_output_tokens, encoder_out, incremental_state, full_context_alignment, alignment_layer, alignment_heads, tgt_src_cursors, tgt_actnode_masks)\u001b[0m\n\u001b[1;32m   1260\u001b[0m     self_attn_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;66;03m# change the decoder layer to output both cross_attention (as in default case)\u001b[39;00m\n\u001b[1;32m   1263\u001b[0m \u001b[38;5;66;03m# and the decoder self attention\u001b[39;00m\n\u001b[0;32m-> 1264\u001b[0m x, layer_attn, _, self_attn \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_out\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder_out\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mencoder_out\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_out\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder_padding_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mencoder_out\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mincremental_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mself_attn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mself_attn_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mneed_attn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43malignment_layer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mneed_head_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43malignment_layer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1273\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# customized\u001b[39;49;00m\n\u001b[1;32m   1274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcross_attention_mask\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtgt_src_align_layers\u001b[49m\n\u001b[1;32m   1276\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1277\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1278\u001b[0m inner_states\u001b[38;5;241m.\u001b[39mappend(x)\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m layer_attn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m idx \u001b[38;5;241m==\u001b[39m alignment_layer:\n",
      "File \u001b[0;32m~/Desktop/vscodeFiles/british/cenv_x86/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/vscodeFiles/british/cenv_x86/lib/python3.8/site-packages/fairseq_ext/modules/transformer_layer.py:399\u001b[0m, in \u001b[0;36mTransformerDecoderLayer.forward\u001b[0;34m(self, x, encoder_out, encoder_padding_mask, incremental_state, prev_self_attn_state, prev_attn_state, self_attn_mask, self_attn_padding_mask, need_attn, need_head_weights, cross_attention_mask, ptr_self_attn_mask, graph_self_attn_mask)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize_before:\n\u001b[1;32m    397\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_layer_norm(x)\n\u001b[0;32m--> 399\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_fn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    400\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_dropout_module(x)\n\u001b[1;32m    401\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n",
      "File \u001b[0;32m~/Desktop/vscodeFiles/british/cenv_x86/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/vscodeFiles/british/cenv_x86/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "output_data = []\n",
    "for i, text in enumerate(sentences):\n",
    "    penmans = []\n",
    "    branches = []\n",
    "    for sentence in text:\n",
    "        # Tokenize the sentence\n",
    "        tokens, positions = parser.tokenize(sentence)\n",
    "        \n",
    "        # Parse the sentence\n",
    "        annotations, machines = parser.parse_sentence(tokens)\n",
    "\n",
    "        # Get AMR graph object\n",
    "        amr = machines.get_amr()\n",
    "        \n",
    "        penman_notation = amr.to_penman(jamr=False, isi=True)\n",
    "        br = len(amr.edges)\n",
    "        \n",
    "        penmans.append(penman_notation)\n",
    "        branches.append(br)\n",
    "\n",
    "    print (f\"Sentence {i}/{len(sentences)}\")\n",
    "\n",
    "    output_data.append({\n",
    "            'penman_notation': penmans,\n",
    "            'branches' : branches\n",
    "        \n",
    "        })\n",
    "\n",
    "print(\"AMR processing completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "penmans = [x['penman_notation'] for x in output_data]\n",
    "branches = [x['branches'] for x in output_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "588"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(penmans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.DataFrame(output_data)\n",
    "output_df.to_csv('../dataset/amr_output.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Austria',\n",
       " 'Belgium',\n",
       " 'Bulgaria',\n",
       " 'Cyprus',\n",
       " 'Czech Republic',\n",
       " 'Denmark',\n",
       " 'England',\n",
       " 'Estonia',\n",
       " 'Finland',\n",
       " 'France',\n",
       " 'Germany',\n",
       " 'Greece',\n",
       " 'Hungary',\n",
       " 'Ireland',\n",
       " 'Italy',\n",
       " 'Latvia',\n",
       " 'Lithuania',\n",
       " 'Luxembourg',\n",
       " 'Malta',\n",
       " 'Netherlands',\n",
       " 'Northern Ireland',\n",
       " 'Poland',\n",
       " 'Portugal',\n",
       " 'Romania',\n",
       " 'Scotland',\n",
       " 'Slovakia',\n",
       " 'Slovenia',\n",
       " 'Spain',\n",
       " 'Sweden',\n",
       " 'Unknown',\n",
       " 'Wales'}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df['state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = ['England','Ireland','Northern Ireland','Wales']\n",
    "filtered_df = df[df['state'].isin(states)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "England: 6053\n",
      "Ireland: 2810\n",
      "Northern Ireland: 459\n",
      "Wales: 193\n"
     ]
    }
   ],
   "source": [
    "for s in states:\n",
    "    print(s + \": \" + str(len(df[df['state']==s])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_df = df[df['state']=='England'].sample(frac=1)\n",
    "irs_df = df[df['state']=='Ireland'].sample(frac=1)\n",
    "nir_df = df[df['state']=='Northern Ireland'].sample(frac=1)\n",
    "wal_df = df[df['state']=='Wales'].sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>euroid</th>\n",
       "      <th>language</th>\n",
       "      <th>name</th>\n",
       "      <th>normalized_name</th>\n",
       "      <th>seq_speaker_id</th>\n",
       "      <th>session_id</th>\n",
       "      <th>type</th>\n",
       "      <th>region</th>\n",
       "      <th>state</th>\n",
       "      <th>missing</th>\n",
       "      <th>reason</th>\n",
       "      <th>da</th>\n",
       "      <th>de</th>\n",
       "      <th>en</th>\n",
       "      <th>es</th>\n",
       "      <th>fr</th>\n",
       "      <th>it</th>\n",
       "      <th>nl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>4554</td>\n",
       "      <td>EN</td>\n",
       "      <td>Davies</td>\n",
       "      <td>Davies</td>\n",
       "      <td>57</td>\n",
       "      <td>ep-00-03-14</td>\n",
       "      <td>speech</td>\n",
       "      <td>Lancashire</td>\n",
       "      <td>England</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nHr. formand, n af glderne ved at tilbringe...</td>\n",
       "      <td>\\nHerr Prsident, eines der Vergngen von Belg...</td>\n",
       "      <td>\\nMr President, one of the joys of spending ti...</td>\n",
       "      <td>\\nSeor Presidente, uno de los placeres que no...</td>\n",
       "      <td>\\nMonsieur le Prsident, l'un des plaisirs que...</td>\n",
       "      <td>\\nSignor Presidente, uno dei piaceri che si gu...</td>\n",
       "      <td>\\nMijnheer de Voorzitter, n van de geneugten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5337</th>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>1428</td>\n",
       "      <td>EN</td>\n",
       "      <td>Balfe (PPE-DE ).</td>\n",
       "      <td>Balfe PPEDE</td>\n",
       "      <td>131</td>\n",
       "      <td>ep-04-04-21</td>\n",
       "      <td>speech</td>\n",
       "      <td>Suffolk</td>\n",
       "      <td>England</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\n\\n - Hr. formand, kan De give Parlamentet ...</td>\n",
       "      <td>\\n\\n  Ich lasse das Parlament darber entsc...</td>\n",
       "      <td>\\n\\n  Mr President, could you give the House...</td>\n",
       "      <td>\\n\\n  Seor Presidente, podra indicar a l...</td>\n",
       "      <td>\\n\\n - Monsieur le Prsident, pourriez-vous ...</td>\n",
       "      <td>\\n\\n  Signor Presidente, potrebbe dare qual...</td>\n",
       "      <td>\\n\\n - Mijnheer de Voorzitter, kunt u ons ie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4155</th>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>4524</td>\n",
       "      <td>EN</td>\n",
       "      <td>Lucas (Verts/ALE)</td>\n",
       "      <td>Lucas VertsALE</td>\n",
       "      <td>144</td>\n",
       "      <td>ep-03-06-03</td>\n",
       "      <td>speech</td>\n",
       "      <td>Worcestershire</td>\n",
       "      <td>England</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nEmnet for denne betnkning er yderst vigtigt...</td>\n",
       "      <td>\\n. (EN) Der vorliegende Bericht befasst sich ...</td>\n",
       "      <td>\\n. The subject of this report is an extremely...</td>\n",
       "      <td>\\n. (EN) El tema de este informe es sumamente ...</td>\n",
       "      <td>\\nLe sujet de ce rapport est de la plus haute ...</td>\n",
       "      <td>\\nL'argomento di questa relazione  estremamen...</td>\n",
       "      <td>\\n. (EN) Dit verslag gaat over een buitengewoo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             country  euroid language               name normalized_name  \\\n",
       "235   United Kingdom    4554       EN             Davies          Davies   \n",
       "5337  United Kingdom    1428       EN   Balfe (PPE-DE ).     Balfe PPEDE   \n",
       "4155  United Kingdom    4524       EN  Lucas (Verts/ALE)  Lucas VertsALE   \n",
       "\n",
       "      seq_speaker_id   session_id    type          region    state  missing  \\\n",
       "235               57  ep-00-03-14  speech      Lancashire  England    False   \n",
       "5337             131  ep-04-04-21  speech         Suffolk  England    False   \n",
       "4155             144  ep-03-06-03  speech  Worcestershire  England    False   \n",
       "\n",
       "      reason                                                 da  \\\n",
       "235      NaN  \\nHr. formand, n af glderne ved at tilbringe...   \n",
       "5337     NaN  \\n\\n - Hr. formand, kan De give Parlamentet ...   \n",
       "4155     NaN  \\nEmnet for denne betnkning er yderst vigtigt...   \n",
       "\n",
       "                                                     de  \\\n",
       "235   \\nHerr Prsident, eines der Vergngen von Belg...   \n",
       "5337  \\n\\n  Ich lasse das Parlament darber entsc...   \n",
       "4155  \\n. (EN) Der vorliegende Bericht befasst sich ...   \n",
       "\n",
       "                                                     en  \\\n",
       "235   \\nMr President, one of the joys of spending ti...   \n",
       "5337  \\n\\n  Mr President, could you give the House...   \n",
       "4155  \\n. The subject of this report is an extremely...   \n",
       "\n",
       "                                                     es  \\\n",
       "235   \\nSeor Presidente, uno de los placeres que no...   \n",
       "5337  \\n\\n  Seor Presidente, podra indicar a l...   \n",
       "4155  \\n. (EN) El tema de este informe es sumamente ...   \n",
       "\n",
       "                                                     fr  \\\n",
       "235   \\nMonsieur le Prsident, l'un des plaisirs que...   \n",
       "5337  \\n\\n - Monsieur le Prsident, pourriez-vous ...   \n",
       "4155  \\nLe sujet de ce rapport est de la plus haute ...   \n",
       "\n",
       "                                                     it  \\\n",
       "235   \\nSignor Presidente, uno dei piaceri che si gu...   \n",
       "5337  \\n\\n  Signor Presidente, potrebbe dare qual...   \n",
       "4155  \\nL'argomento di questa relazione  estremamen...   \n",
       "\n",
       "                                                     nl  \n",
       "235   \\nMijnheer de Voorzitter, n van de geneugten...  \n",
       "5337  \\n\\n - Mijnheer de Voorzitter, kunt u ons ie...  \n",
       "4155  \\n. (EN) Dit verslag gaat over een buitengewoo...  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ex_per_class = 193"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_df = eng_df.head(num_ex_per_class)\n",
    "irs_df = irs_df.head(num_ex_per_class)\n",
    "nir_df = nir_df.head(num_ex_per_class)\n",
    "wal_df = wal_df.head(num_ex_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_first_texts(texts):\n",
    "    if(texts[0] == \"\\n.\"):\n",
    "        texts = texts[1:]\n",
    "    l = len(texts[0])\n",
    "    texts_list = [texts[0]]\n",
    "    i = 1\n",
    "    while i < len(texts) and l + len(texts[i]) <= 500:\n",
    "        texts_list.append(texts[i])\n",
    "        l+= len(texts[i])\n",
    "        i+=1\n",
    "    return texts_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentence_samples(row):\n",
    "    return choose_first_texts(splits(row['en']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat(lists):\n",
    "    l = []\n",
    "    for lis in lists:\n",
    "        l+=lis\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_sentences = eng_df.apply(extract_sentence_samples, axis=1)\n",
    "eng_sentences = concat(eng_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "irs_sentences = concat(irs_df.apply(extract_sentence_samples, axis=1))\n",
    "nir_sentences = concat(nir_df.apply(extract_sentence_samples, axis=1))\n",
    "wal_sentences = concat(wal_df.apply(extract_sentence_samples, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "638\n",
      "666\n",
      "635\n",
      "603\n"
     ]
    }
   ],
   "source": [
    "sentences = [eng_sentences,irs_sentences,nir_sentences,wal_sentences]\n",
    "for s in sentences:\n",
    "    print(str(len(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = len(eng_sentences) * ['eng'] + len(irs_sentences) * ['irs'] + len(nir_sentences) * ['nir'] + len(wal_sentences) * ['wal']\n",
    "sentence = concat(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = [s.strip('\\n\\xa0') for s in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_df = pd.DataFrame({'sentence' : sentence, 'label' : label}).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               sentence label\n",
      "762    Like previous speakers, I welcome it because ...   irs\n",
      "1708                    This is a very worrying aspect.   nir\n",
      "1796  Mr President, recently, along with other passe...   nir\n",
      "1162  Mr President, I am speaking basically on the A...   irs\n",
      "546    Mr President, the Commissioner is of course n...   eng\n"
     ]
    }
   ],
   "source": [
    "print(class_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_df.to_csv('../dataset/classify_df.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "britishKernel",
   "language": "python",
   "name": "britishkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
